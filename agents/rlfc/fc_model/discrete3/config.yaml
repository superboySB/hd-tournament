actor:
  aid: 0
  algorithm: sync-ppo
  device: cpu
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  name: sync-ppo
  print_for_debug: false
  rms:
    obs:
      normalize_obs: true
      obs_clip: 10
      obs_names:
      - obs
      - global_state
      obs_normalized_axis:
      - 0
      - 1
      - 2
      obs_normalized_ndim: 1
      use_feature_mask: true
    reward:
      gamma: 0.995
      normalize_reward: false
      reward_normalized_axis:
      - 0
      - 1
      - 2
      reward_normalized_ndim: 0
      update_reward_rms_in_time: false
  root_dir: logs\hd-fc\sync-ppo-th
  seed: 0
  update_obs_at_execution: false
  update_obs_rms_at_execution: false
agent:
  aid: 0
  algorithm: sync-ppo
  device: cpu
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  name: sync-ppo
  print_for_debug: false
  root_dir: logs\hd-fc\sync-ppo-th
  seed: 0
aid: 0
algorithm: sync-ppo
buffer:
  aid: 0
  algorithm: sync-ppo
  device: cpu
  gamma: 0.995
  lam: 0.95
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  n_envs: 1
  n_runners: 40
  n_steps: 100
  name: sync-ppo
  print_for_debug: false
  queue_size: 2
  root_dir: logs\hd-fc\sync-ppo-th
  sample_keys:
  - raw_obs
  - obs
  - raw_global_state
  - global_state
  - action
  - prev_info
  - value
  - reward
  - discount
  - reset
  - sample_mask
  - mu_logprob
  - mu_logits
  - state_reset
  - state
  seed: 0
  timeout_done: false
  type: ac
controller:
  aid: 0
  algorithm: sync-ppo
  device: cpu
  initialize_rms: true
  max_pbt_iterations: 1
  max_steps_per_iteration: 1000000000
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  name: sync-ppo
  print_for_debug: false
  restart_runners_period: null
  root_dir: logs\hd-fc\sync-ppo-th
  seed: 0
  store_period: 100000
date: 0803
device: cpu
dllib: th
env:
  aid: 0
  algorithm: sync-ppo
  attitude_exp_scale: 20
  attitude_reward_scale: 0.05
  dead_reward: 50
  delta_range:
  - - -100
    - -30
    - -40
  - - 100
    - 30
    - 40
  device: cpu
  env_name: hd-fc
  fc_target_steps: 500
  low_height_scale: 0.1
  low_speed_scale: 0.1
  max_episode_steps: 3000
  met_reward: 50
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  n_bins: 41
  n_envs: 1
  name: sync-ppo
  pitch_init: 90
  print_for_debug: false
  root_dir: logs\hd-fc\sync-ppo-th
  scen: scen.json
  seed: 0
  suite: hd
  target_limit:
  - - 1000
    - 300
  - - 14500
    - 450
  target_threshold:
  - 20
  - 5
  - 10
  task_exp_scale:
  - 50
  - 15
  - 20
  task_reward_scale: 0.1
  timeout_done: false
  total_reward_scale: 0.1
env_stats:
  action_dim:
  - action_aileron: 41
    action_elevator: 41
    action_rudder: 41
    action_throttle: 41
  action_high: 1
  action_low: -1
  action_shape:
  - action_aileron: []
    action_elevator: []
    action_rudder: []
    action_throttle: []
  aid2gids:
  - - 0
  aid2uids:
  - - 0
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
    - 9
    - 10
    - 11
  feature_mask: null
  gid2uids:
  - - 0
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
    - 9
    - 10
    - 11
  is_action_discrete:
  - action_aileron: true
    action_elevator: true
    action_rudder: true
    action_throttle: true
  is_multi_agent: true
  is_simultaneous_move: true
  life_long: false
  max_episode_steps: 3000
  n_agents: 1
  n_envs: 1
  n_runners: 1
  n_units: 12
  obs_keys:
  - - obs
    - global_state
    - sample_mask
  obs_shape:
  - global_state:
    - 16
    obs:
    - 16
    sample_mask: []
  uid2aid:
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  uid2gid:
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  - 0
  use_action_mask:
  - action_aileron: false
    action_elevator: false
    action_rudder: false
    action_throttle: false
  use_sample_mask: true
info: ''
iteration: 1
launch_time: '2024-08-03 12:27:19'
local_mode: false
loss:
  adv_type: gae
  aid: 0
  algorithm: sync-ppo
  c_clip: 1
  device: cpu
  huber_threshold: 10
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  name: sync-ppo
  norm_adv: true
  pg_type: ppo
  policy_sample_mask: true
  popart: true
  ppo_clip_range: 0.2
  print_for_debug: false
  prnn_bptt: 10
  rho_clip: 1
  root_dir: logs\hd-fc\sync-ppo-th
  seed: 0
  stats:
    entropy_coef: 0.001
    gamma: 0.995
    lam: 0.95
    pg_coef: 1
    value_coef: 1
  target_type: gae
  value_clip_range: 0.2
  value_loss: clip_huber
  value_sample_mask: true
  vrnn_bptt: 10
model:
  aid: 0
  algorithm: sync-ppo
  device: cpu
  gamma: 0.995
  joint_log_prob: false
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  name: sync-ppo
  policy:
    activation: relu
    init_std: 0.2
    nn_id: policy
    norm: null
    norm_after_activation: true
    out_act: tanh
    out_scale: 0.01
    rnn_init: orthogonal
    rnn_layers: 1
    rnn_norm: null
    rnn_type: null
    rnn_units: 64
    sigmoid_scale: true
    std_x_coef: 1
    std_y_coef: 0.5
    units_list:
    - 64
    - 64
    use_feature_norm: false
    w_init: orthogonal
  print_for_debug: false
  root_dir: logs\hd-fc\sync-ppo-th
  seed: 0
  value:
    activation: relu
    nn_id: value
    norm: null
    norm_after_activation: true
    rnn_init: orthogonal
    rnn_norm: null
    rnn_type: null
    rnn_units: 64
    units_list:
    - 64
    - 64
    use_feature_norm: false
    w_init: orthogonal
  vnorm:
    axis:
    - 0
    - 1
    nn_id: vnorm
model_info: n_bins=41
model_name: 0803\n_bins=41\seed=None\a0\i1-v1
model_source: '0: raw'
monitor:
  aid: 0
  algorithm: sync-ppo
  device: cpu
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  name: sync-ppo
  print_for_debug: false
  root_dir: logs\hd-fc\sync-ppo-th
  seed: 0
  self_play: null
  use_tensorboard: true
n_agents: 1
n_bins: 41
name: sync-ppo
parameter_server:
  aid: 0
  algorithm: sync-ppo
  device: cpu
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  name: sync-ppo
  online_frac: 0.2
  payoff:
    sampling_strategy:
      p: 1
      type: pfsp
    step_size: 0.01
    update_interval: 180
  print_for_debug: false
  root_dir: logs\hd-fc\sync-ppo-th
  seed: 0
  self_play: null
  train_from_scratch_frac: 1
precision: 32
print_for_debug: false
ray_config:
  agent:
    num_gpus: 1
  aid: 0
  algorithm: sync-ppo
  device: cpu
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  name: sync-ppo
  print_for_debug: false
  root_dir: logs\hd-fc\sync-ppo-th
  runner:
    num_cpus: 1
    num_gpus: 0
  seed: 0
root_dir: logs\hd-fc\sync-ppo-th
runner:
  aid: 0
  algorithm: sync-ppo
  device: cpu
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  n_runners: 40
  n_steps: 100
  name: sync-ppo
  print_for_debug: false
  push_every_episode: false
  root_dir: logs\hd-fc\sync-ppo-th
  seed: 0
  self_play: null
seed: 0
status: training
strategy:
  aid: 0
  algorithm: sync-ppo
  device: cpu
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  name: sync-ppo
  print_for_debug: false
  root_dir: logs\hd-fc\sync-ppo-th
  seed: 0
  train_loop: {}
trainer:
  aid: 0
  algorithm: sync-ppo
  clip_norm: 10
  debug: false
  device: cpu
  model_name: 0803\n_bins=41\seed=None\a0\i1-v1
  n_envs: 1
  n_epochs: 5
  n_mbs: 1
  n_runners: 40
  n_steps: 100
  name: sync-ppo
  policy_opt:
    eps: 1.0e-05
    lr: 0.0003
    opt_name: Adam
  popart: true
  print_for_debug: false
  root_dir: logs\hd-fc\sync-ppo-th
  seed: 0
  value_opt:
    eps: 1.0e-05
    lr: 0.0003
    opt_name: Adam
version: 1
